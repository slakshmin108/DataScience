{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/lakshmi/spark-2.4.5-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My_Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a schema for reading input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DecimalType, FloatType, IntegerType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([StructField(\"country\", StringType(), False), StructField(\"area\", DecimalType(10,2), False),\n",
    "                     StructField(\"female\", IntegerType(), True), StructField(\"male\", IntegerType(), True), \n",
    "                     StructField(\"population\", IntegerType(), True), StructField(\"density\", FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country', 'area', 'female', 'male', 'population', 'density']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schema.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from a CSV into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/media/sf_Ubuntu_Shared/popdensity.csv', schema=data_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- area: decimal(10,2) (nullable = true)\n",
      " |-- female: integer (nullable = true)\n",
      " |-- male: integer (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- density: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country', 'area', 'female', 'male', 'population', 'density']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+----+----------+----------+\n",
      "|        country|    area|female|male|population|   density|\n",
      "+---------------+--------+------+----+----------+----------+\n",
      "|BadenWrttemberg|35751.65|  5465|5271|     10736|0.30029383|\n",
      "|         Bayern|70551.57|  6366|6103|     12469|0.17673597|\n",
      "|         Berlin|  891.85|  1736|1660|      3396| 3.8078153|\n",
      "|    Brandenburg|29478.61|  1293|1267|      2560|0.08684263|\n",
      "|         Bremen|  404.28|   342| 321|       663| 1.6399525|\n",
      "+---------------+--------+------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'country'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['country'] # this returns a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'country'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             country|\n",
      "+--------------------+\n",
      "|     BadenWrttemberg|\n",
      "|              Bayern|\n",
      "|              Berlin|\n",
      "|         Brandenburg|\n",
      "|              Bremen|\n",
      "|             Hamburg|\n",
      "|              Hessen|\n",
      "|Mecklenburg-Vorpo...|\n",
      "|       Niedersachsen|\n",
      "| Nordrhein-Westfalen|\n",
      "|     Rheinland-Pfalz|\n",
      "|            Saarland|\n",
      "|             Sachsen|\n",
      "|      Sachsen-Anhalt|\n",
      "|  Schleswig-Holstein|\n",
      "|           Thuringen|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('country')) # select() method returns a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             country|population|\n",
      "+--------------------+----------+\n",
      "|     BadenWrttemberg|     10736|\n",
      "|              Bayern|     12469|\n",
      "|              Berlin|      3396|\n",
      "|         Brandenburg|      2560|\n",
      "|              Bremen|       663|\n",
      "|             Hamburg|      1743|\n",
      "|              Hessen|      6092|\n",
      "|Mecklenburg-Vorpo...|      1707|\n",
      "|       Niedersachsen|      7994|\n",
      "| Nordrhein-Westfalen|     18058|\n",
      "|     Rheinland-Pfalz|      4059|\n",
      "|            Saarland|      1050|\n",
      "|             Sachsen|      4274|\n",
      "|      Sachsen-Anhalt|      2470|\n",
      "|  Schleswig-Holstein|      2833|\n",
      "|           Thuringen|      2335|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('country', 'population').show() # selecting multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[country: string, population: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['country', 'population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|             country|Population in Thousands|\n",
      "+--------------------+-----------------------+\n",
      "|     BadenWrttemberg|                 10.736|\n",
      "|              Bayern|                 12.469|\n",
      "|              Berlin|                  3.396|\n",
      "|         Brandenburg|                   2.56|\n",
      "|              Bremen|                  0.663|\n",
      "|             Hamburg|                  1.743|\n",
      "|              Hessen|                  6.092|\n",
      "|Mecklenburg-Vorpo...|                  1.707|\n",
      "|       Niedersachsen|                  7.994|\n",
      "| Nordrhein-Westfalen|                 18.058|\n",
      "|     Rheinland-Pfalz|                  4.059|\n",
      "|            Saarland|                   1.05|\n",
      "|             Sachsen|                  4.274|\n",
      "|      Sachsen-Anhalt|                   2.47|\n",
      "|  Schleswig-Holstein|                  2.833|\n",
      "|           Thuringen|                  2.335|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.country, (df.population/1000).alias('Population in Thousands')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='BadenWrttemberg', area=Decimal('35751.65'), female=5465, male=5271, population=10736, density=0.30029383301734924),\n",
       " Row(country='Bayern', area=Decimal('70551.57'), female=6366, male=6103, population=12469, density=0.17673596739768982),\n",
       " Row(country='Berlin', area=Decimal('891.85'), female=1736, male=1660, population=3396, density=3.8078153133392334),\n",
       " Row(country='Brandenburg', area=Decimal('29478.61'), female=1293, male=1267, population=2560, density=0.0868426263332367),\n",
       " Row(country='Bremen', area=Decimal('404.28'), female=342, male=321, population=663, density=1.639952540397644)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5) # returns the first n rows; this method should be used only if the df is small enough to be loaded in driver's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_row = df.head(1) #accessing a single row of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='BadenWrttemberg', area=Decimal('35751.65'), female=5465, male=5271, population=10736, density=0.30029383301734924)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10736"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_row[0]['population'] # accessing a column from the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decimal('35751.65')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_row[0]['area'] # accessing a column from the row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding / renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('diff', df.female-df.male) #adding a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+----+----------+----------+----+\n",
      "|             country|    area|female|male|population|   density|diff|\n",
      "+--------------------+--------+------+----+----------+----------+----+\n",
      "|     BadenWrttemberg|35751.65|  5465|5271|     10736|0.30029383| 194|\n",
      "|              Bayern|70551.57|  6366|6103|     12469|0.17673597| 263|\n",
      "|              Berlin|  891.85|  1736|1660|      3396| 3.8078153|  76|\n",
      "|         Brandenburg|29478.61|  1293|1267|      2560|0.08684263|  26|\n",
      "|              Bremen|  404.28|   342| 321|       663| 1.6399525|  21|\n",
      "|             Hamburg|  755.16|   894| 849|      1743| 2.3081203|  45|\n",
      "|              Hessen|21114.79|  3109|2983|      6092|0.28851813| 126|\n",
      "|Mecklenburg-Vorpo...|23180.14|   861| 846|      1707|0.07364062|  15|\n",
      "|       Niedersachsen|47624.20|  4076|3918|      7994|0.16785584| 158|\n",
      "| Nordrhein-Westfalen|34085.29|  9261|8797|     18058| 0.5297887| 464|\n",
      "|     Rheinland-Pfalz|19853.36|  2069|1990|      4059|0.20444901|  79|\n",
      "|            Saarland| 2568.70|   540| 510|      1050|0.40876707|  30|\n",
      "|             Sachsen|18415.51|  2191|2083|      4274|0.23208697| 108|\n",
      "|      Sachsen-Anhalt|20446.31|  1264|1206|      2470|0.12080419|  58|\n",
      "|  Schleswig-Holstein|15799.38|  1448|1385|      2833|0.17931083|  63|\n",
      "|           Thuringen|16172.10|  1185|1150|      2335|0.14438446|  35|\n",
      "+--------------------+--------+------+----+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+----+----------+----------+------------------+\n",
      "|             country|    area|female|male|population|   density|diff_in_population|\n",
      "+--------------------+--------+------+----+----------+----------+------------------+\n",
      "|     BadenWrttemberg|35751.65|  5465|5271|     10736|0.30029383|               194|\n",
      "|              Bayern|70551.57|  6366|6103|     12469|0.17673597|               263|\n",
      "|              Berlin|  891.85|  1736|1660|      3396| 3.8078153|                76|\n",
      "|         Brandenburg|29478.61|  1293|1267|      2560|0.08684263|                26|\n",
      "|              Bremen|  404.28|   342| 321|       663| 1.6399525|                21|\n",
      "|             Hamburg|  755.16|   894| 849|      1743| 2.3081203|                45|\n",
      "|              Hessen|21114.79|  3109|2983|      6092|0.28851813|               126|\n",
      "|Mecklenburg-Vorpo...|23180.14|   861| 846|      1707|0.07364062|                15|\n",
      "|       Niedersachsen|47624.20|  4076|3918|      7994|0.16785584|               158|\n",
      "| Nordrhein-Westfalen|34085.29|  9261|8797|     18058| 0.5297887|               464|\n",
      "|     Rheinland-Pfalz|19853.36|  2069|1990|      4059|0.20444901|                79|\n",
      "|            Saarland| 2568.70|   540| 510|      1050|0.40876707|                30|\n",
      "|             Sachsen|18415.51|  2191|2083|      4274|0.23208697|               108|\n",
      "|      Sachsen-Anhalt|20446.31|  1264|1206|      2470|0.12080419|                58|\n",
      "|  Schleswig-Holstein|15799.38|  1448|1385|      2833|0.17931083|                63|\n",
      "|           Thuringen|16172.10|  1185|1150|      2335|0.14438446|                35|\n",
      "+--------------------+--------+------+----+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumnRenamed('diff', 'diff_in_population').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL commands using dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+----+----------+----------+\n",
      "|            country|    area|female|male|population|   density|\n",
      "+-------------------+--------+------+----+----------+----------+\n",
      "|    BadenWrttemberg|35751.65|  5465|5271|     10736|0.30029383|\n",
      "|             Bayern|70551.57|  6366|6103|     12469|0.17673597|\n",
      "|Nordrhein-Westfalen|34085.29|  9261|8797|     18058| 0.5297887|\n",
      "+-------------------+--------+------+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM Population WHERE population>10000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = df.collect() # collect() returns the rows of dataframe as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L1) # note that this is same as df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(country='BadenWrttemberg', area=Decimal('35751.65'), female=5465, male=5271, population=10736, density=0.30029383301734924)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1[0] # first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering data based on conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType([StructField(\"Date\", DateType(), True), StructField(\"Open\", FloatType(), True),\n",
    "                      StructField(\"High\", FloatType(), True), StructField(\"Low\", FloatType(), True),\n",
    "                      StructField(\"Close\", FloatType(), True), StructField(\"Volume\", IntegerType(), True),\n",
    "                      StructField(\"Adj_Close\", FloatType(), True) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(path=\"/media/sf_Ubuntu_Shared/appl_stock.csv\", schema=schema2, header=True) #DateType format is not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(path=\"/media/sf_Ubuntu_Shared/appl_stock.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj_Close: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|      Date|     Open|  High|   Low|    Close|   Volume|Adj_Close|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "|2010-01-04|   213.43| 214.5|212.38|   214.01|123432400| 27.72704|\n",
      "|2010-01-05|214.59999|215.59|213.25|214.37999|150476200|27.774977|\n",
      "|2010-01-06|214.37999|215.23|210.75|   210.97|138040000|27.333178|\n",
      "|2010-01-07|   211.75| 212.0|209.05|   210.58|119282800| 27.28265|\n",
      "|2010-01-08|210.29999| 212.0|209.06|211.98001|111902700|27.464033|\n",
      "+----------+---------+------+------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+-------------------+------------------+\n",
      "|summary|              Open|             High|               Low|             Close|             Volume|         Adj_Close|\n",
      "+-------+------------------+-----------------+------------------+------------------+-------------------+------------------+\n",
      "|  count|              1762|             1762|              1762|              1762|               1762|              1762|\n",
      "|   mean|313.07631053340015|315.9112879420788| 309.8282404974289| 312.9270658330668|9.422577587968218E7| 75.00174113976158|\n",
      "| stddev|185.29946731081264|186.8981766989906|183.38391663940038|185.14710364848838|6.020518776592709E7|28.574929738834484|\n",
      "|    min|              90.0|             90.7|             89.47|             90.28|           11475900|         24.881912|\n",
      "|    max|            702.41|           705.07|            699.57|         702.10004|          470249500|         127.96609|\n",
      "+-------+------------------+-----------------+------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering using SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "|      Date|     Open|     High|      Low|    Close|   Volume|Adj_Close|\n",
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "|2012-02-14|504.65997|509.56003|    502.0|   509.46|115099600| 66.00541|\n",
      "|2012-02-15|   514.26|526.29004|496.88998|497.66998|376530000|  64.4779|\n",
      "|2012-02-17|   503.11|507.77002|    500.3|   502.12|133951300| 65.05444|\n",
      "|2012-02-21|   506.88|514.85004|   504.12|514.85004|151398800|66.703735|\n",
      "|2012-02-22|   513.08|   515.49|509.07004|   513.04|120825600| 66.46923|\n",
      "+----------+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(\"Open > 500\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(\"Open > 500\").count() # using SQL format expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1['Open'] > 500).count() # using python expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter( (df1['Open'] >= 500) & (df1['Open'] < 510) ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter( (\"Open >= 500\") and (\"Open < 510\") ).count() #Incorrect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('Date', 'Open', 'Close').filter(\"Open >= 500\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Unlike SQL queries the order of select and where clauses can be interchanged\n",
    "df1.filter(\"Open >= 500\").select('Date', 'Open', 'Close').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"appleStock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from appleStock where (Open >= 500) and (Open < 510)\").count() # SQL query works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df1.filter( (df1['Open'] >= 500) & (df1['Open'] < 510) ).collect() # return a list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2012, 2, 14), Open=504.65997314453125, High=509.5600280761719, Low=502.0, Close=509.4599914550781, Volume=115099600, Adj_Close=66.00540924072266),\n",
       " Row(Date=datetime.date(2012, 2, 17), Open=503.1099853515625, High=507.77001953125, Low=500.29998779296875, Close=502.1199951171875, Volume=133951300, Adj_Close=65.054443359375),\n",
       " Row(Date=datetime.date(2012, 2, 21), Open=506.8800048828125, High=514.8500366210938, Low=504.1199951171875, Close=514.8500366210938, Volume=151398800, Adj_Close=66.7037353515625),\n",
       " Row(Date=datetime.date(2012, 12, 17), Open=508.92999267578125, High=520.0, Low=501.2300109863281, Close=518.8299560546875, Volume=189401800, Adj_Close=67.81631469726562),\n",
       " Row(Date=datetime.date(2013, 1, 14), Open=502.6800231933594, High=507.5, Low=498.510009765625, Close=501.75, Volume=183551900, Adj_Close=65.58379364013672),\n",
       " Row(Date=datetime.date(2013, 1, 22), Open=504.5600280761719, High=507.8799743652344, Low=496.6300048828125, Close=504.7699890136719, Volume=115386600, Adj_Close=65.9785385131836),\n",
       " Row(Date=datetime.date(2013, 1, 23), Open=508.80999755859375, High=514.989990234375, Low=504.7699890136719, Close=514.010009765625, Volume=215377400, Adj_Close=67.18629455566406),\n",
       " Row(Date=datetime.date(2013, 8, 16), Open=500.1499938964844, High=502.94000244140625, Low=498.8600158691406, Close=502.33001708984375, Volume=90576500, Adj_Close=66.91834259033203),\n",
       " Row(Date=datetime.date(2013, 8, 19), Open=504.3399658203125, High=513.739990234375, Low=504.0, Close=507.739990234375, Volume=127629600, Adj_Close=67.63904571533203),\n",
       " Row(Date=datetime.date(2013, 8, 20), Open=509.7099914550781, High=510.5699768066406, Low=500.82000732421875, Close=501.07000732421875, Volume=89672100, Adj_Close=66.75049591064453),\n",
       " Row(Date=datetime.date(2013, 8, 21), Open=503.5899963378906, High=507.1499938964844, Low=501.1999816894531, Close=502.3600158691406, Volume=83969900, Adj_Close=66.9223403930664),\n",
       " Row(Date=datetime.date(2013, 8, 22), Open=504.9800109863281, High=505.5899963378906, Low=498.1999816894531, Close=502.9599609375, Volume=61051900, Adj_Close=67.00226593017578),\n",
       " Row(Date=datetime.date(2013, 8, 23), Open=503.2699890136719, High=503.3500061035156, Low=499.3500061035156, Close=501.02001953125, Volume=55682900, Adj_Close=66.74383544921875),\n",
       " Row(Date=datetime.date(2013, 8, 26), Open=500.75, High=510.1999816894531, Low=500.5, Close=502.9700012207031, Volume=82741400, Adj_Close=67.00360870361328),\n",
       " Row(Date=datetime.date(2013, 9, 5), Open=500.25, High=500.6799621582031, Low=493.6399841308594, Close=495.2699890136719, Volume=59091900, Adj_Close=65.97783660888672),\n",
       " Row(Date=datetime.date(2013, 9, 9), Open=505.0000305175781, High=507.91998291015625, Low=503.47998046875, Close=506.16998291015625, Volume=85171800, Adj_Close=67.42989349365234),\n",
       " Row(Date=datetime.date(2013, 9, 10), Open=506.20001220703125, High=507.45001220703125, Low=489.5, Close=494.6399841308594, Volume=185798900, Adj_Close=65.89391326904297),\n",
       " Row(Date=datetime.date(2013, 10, 16), Open=500.78997802734375, High=502.5299987792969, Low=499.2300109863281, Close=501.1100158691406, Volume=62775300, Adj_Close=66.75582122802734),\n",
       " Row(Date=datetime.date(2013, 10, 18), Open=505.989990234375, High=509.2599792480469, Low=505.7099914550781, Close=508.8899841308594, Volume=72635500, Adj_Close=67.792236328125),\n",
       " Row(Date=datetime.date(2014, 1, 28), Open=508.760009765625, High=515.0, Low=502.0700378417969, Close=506.4999694824219, Volume=266380800, Adj_Close=67.86779022216797),\n",
       " Row(Date=datetime.date(2014, 1, 29), Open=503.95001220703125, High=507.3699951171875, Low=498.6199951171875, Close=500.75, Volume=125702500, Adj_Close=67.09732818603516),\n",
       " Row(Date=datetime.date(2014, 1, 30), Open=502.53997802734375, High=506.4999694824219, Low=496.70001220703125, Close=499.7799987792969, Volume=169625400, Adj_Close=66.96735382080078),\n",
       " Row(Date=datetime.date(2014, 2, 3), Open=502.6100158691406, High=507.7300109863281, Low=499.3000183105469, Close=501.5299987792969, Volume=100366000, Adj_Close=67.20184326171875),\n",
       " Row(Date=datetime.date(2014, 2, 4), Open=505.8499755859375, High=509.4599914550781, Low=502.760009765625, Close=508.7900085449219, Volume=94170300, Adj_Close=68.17464447021484),\n",
       " Row(Date=datetime.date(2014, 2, 5), Open=506.55999755859375, High=515.2799682617188, Low=506.25, Close=512.5899658203125, Volume=82086200, Adj_Close=68.6838150024414)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.date(2012, 2, 14), Open=504.65997314453125, High=509.5600280761719, Low=502.0, Close=509.4599914550781, Volume=115099600, Adj_Close=66.00540924072266)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504.65997314453125"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['Open'] #accessing an element of the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509.5600280761719"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].High # another method of accessing row element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': '14/02/12',\n",
       " 'Open': 504.659988,\n",
       " 'High': 509.56002,\n",
       " 'Low': 502.000008,\n",
       " 'Close': 509.459991,\n",
       " 'Volume': 115099600,\n",
       " 'Adj Close': 66.005408}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].asDict() # create a dictionary from a row object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"/media/sf_Ubuntu_Shared/sales_details.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Geo: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Profit: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fc623a735f8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy('Company') #groupBy returns a GroupedData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Company: string, sum(Sales): bigint, sum(Profit): bigint]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy('Company').sum('Sales', 'Profit') #aggregate function returns a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|Company|sum(Sales)|sum(Profit)|\n",
      "+-------+----------+-----------+\n",
      "|   APPL|      3700|        441|\n",
      "|     FB|      2388|        535|\n",
      "| GOOGLE|      1961|        291|\n",
      "+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company').sum('Sales', 'Profit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+------------------+\n",
      "|Company| Geo|        avg(Sales)|       avg(Profit)|\n",
      "+-------+----+------------------+------------------+\n",
      "|   APPL|APAC|             340.0|45.666666666666664|\n",
      "|   APPL|EMEA| 503.3333333333333|59.666666666666664|\n",
      "|   APPL|  US|             390.0|41.666666666666664|\n",
      "|     FB|APAC|             173.0|              75.0|\n",
      "|     FB|EMEA| 345.3333333333333|              68.0|\n",
      "|     FB|  US| 277.6666666666667|35.333333333333336|\n",
      "| GOOGLE|APAC|190.66666666666666|              43.0|\n",
      "| GOOGLE|EMEA| 278.6666666666667|32.333333333333336|\n",
      "| GOOGLE|  US|184.33333333333334|21.666666666666668|\n",
      "+-------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Geo').mean('Sales', 'Profit').sort('Company', 'Geo').show() # groupBy multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+------------------+\n",
      "|Company| Geo|        avg(Sales)|       avg(Profit)|\n",
      "+-------+----+------------------+------------------+\n",
      "| GOOGLE|  US|184.33333333333334|21.666666666666668|\n",
      "| GOOGLE|EMEA| 278.6666666666667|32.333333333333336|\n",
      "|     FB|  US| 277.6666666666667|35.333333333333336|\n",
      "|   APPL|  US|             390.0|41.666666666666664|\n",
      "| GOOGLE|APAC|190.66666666666666|              43.0|\n",
      "|   APPL|APAC|             340.0|45.666666666666664|\n",
      "|   APPL|EMEA| 503.3333333333333|59.666666666666664|\n",
      "|     FB|EMEA| 345.3333333333333|              68.0|\n",
      "|     FB|APAC|             173.0|              75.0|\n",
      "+-------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Geo').mean('Sales', 'Profit').sort('avg(Profit)').show() # sorting by a column not grouped by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|sum(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "| GOOGLE|2017|       575|         95|\n",
      "| GOOGLE|2018|       685|        137|\n",
      "| GOOGLE|2019|       701|         59|\n",
      "|     FB|2017|       440|         75|\n",
      "|     FB|2018|      1200|        245|\n",
      "|     FB|2019|       748|        215|\n",
      "|   APPL|2017|       505|        100|\n",
      "|   APPL|2018|      1350|        140|\n",
      "|   APPL|2019|      1845|        201|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Year').sum('Sales', 'Profit').sort(['Company', 'Year'],ascending=[False, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Company='GOOGLE', Year=2017, sum(Sales)=575, sum(Profit)=95),\n",
       " Row(Company='GOOGLE', Year=2018, sum(Sales)=685, sum(Profit)=137),\n",
       " Row(Company='GOOGLE', Year=2019, sum(Sales)=701, sum(Profit)=59),\n",
       " Row(Company='FB', Year=2017, sum(Sales)=440, sum(Profit)=75),\n",
       " Row(Company='FB', Year=2018, sum(Sales)=1200, sum(Profit)=245),\n",
       " Row(Company='FB', Year=2019, sum(Sales)=748, sum(Profit)=215),\n",
       " Row(Company='APPL', Year=2017, sum(Sales)=505, sum(Profit)=100),\n",
       " Row(Company='APPL', Year=2018, sum(Sales)=1350, sum(Profit)=140),\n",
       " Row(Company='APPL', Year=2019, sum(Sales)=1845, sum(Profit)=201)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Year').sum('Sales', 'Profit').sort(['Company', 'Year'],ascending=[False, True]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|max(Sales)|max(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "| GOOGLE|2019|       322|         24|\n",
      "| GOOGLE|2018|       330|         80|\n",
      "| GOOGLE|2017|       250|         40|\n",
      "|     FB|2019|       526|        124|\n",
      "|     FB|2018|       500|        125|\n",
      "|     FB|2017|       230|         35|\n",
      "|   APPL|2019|       760|         89|\n",
      "|   APPL|2018|       600|         60|\n",
      "|   APPL|2017|       200|         45|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Year').max('Sales', 'Profit').orderBy(['Company', 'Year'], ascending=[False,False]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|max(Sales)|min(Profit)|\n",
      "+----------+-----------+\n",
      "|       760|         10|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.agg({'Sales':'max', 'Profit':'min'}).show() #Aggregate function on the entire dataframe;\n",
    "# note that a different function is called for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|max(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "|   APPL|2017|       505|         45|\n",
      "|   APPL|2018|      1350|         60|\n",
      "|   APPL|2019|      1845|         89|\n",
      "|     FB|2017|       440|         35|\n",
      "|     FB|2018|      1200|        125|\n",
      "|     FB|2019|       748|        124|\n",
      "| GOOGLE|2017|       575|         40|\n",
      "| GOOGLE|2018|       685|         80|\n",
      "| GOOGLE|2019|       701|         24|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Year').agg({'Sales':'sum', 'Profit':'max'}).sort(['Company', 'Year']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Company='APPL', Year=2017, sum(Sales)=505, max(Profit)=45),\n",
       " Row(Company='APPL', Year=2018, sum(Sales)=1350, max(Profit)=60),\n",
       " Row(Company='APPL', Year=2019, sum(Sales)=1845, max(Profit)=89),\n",
       " Row(Company='FB', Year=2017, sum(Sales)=440, max(Profit)=35),\n",
       " Row(Company='FB', Year=2018, sum(Sales)=1200, max(Profit)=125),\n",
       " Row(Company='FB', Year=2019, sum(Sales)=748, max(Profit)=124),\n",
       " Row(Company='GOOGLE', Year=2017, sum(Sales)=575, max(Profit)=40),\n",
       " Row(Company='GOOGLE', Year=2018, sum(Sales)=685, max(Profit)=80),\n",
       " Row(Company='GOOGLE', Year=2019, sum(Sales)=701, max(Profit)=24)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy('Company', 'Year').agg({'Sales':'sum', 'Profit':'max'}).sort(['Company', 'Year']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('salesDetails') # create a view for SQL querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = spark.sql(\"select sum(Sales), sum(Profit) from salesDetails where Company = 'GOOGLE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|sum(Sales)|sum(Profit)|\n",
      "+----------+-----------+\n",
      "|      1961|        291|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = spark.sql(\"select Company, Year, sum(Sales), sum(Profit) from salesDetails group by Company, Year order by Company ASC, Year DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|sum(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "|   APPL|2019|      1845|        201|\n",
      "|   APPL|2018|      1350|        140|\n",
      "|   APPL|2017|       505|        100|\n",
      "|     FB|2019|       748|        215|\n",
      "|     FB|2018|      1200|        245|\n",
      "|     FB|2017|       440|         75|\n",
      "| GOOGLE|2019|       701|         59|\n",
      "| GOOGLE|2018|       685|        137|\n",
      "| GOOGLE|2017|       575|         95|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|sum(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "|   APPL|2019|      1845|        201|\n",
      "|   APPL|2018|      1350|        140|\n",
      "|   APPL|2017|       505|        100|\n",
      "|     FB|2019|       748|        215|\n",
      "|     FB|2018|      1200|        245|\n",
      "|     FB|2017|       440|         75|\n",
      "| GOOGLE|2019|       701|         59|\n",
      "| GOOGLE|2018|       685|        137|\n",
      "| GOOGLE|2017|       575|         95|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(['Company', 'Year']).sum('Sales', 'Profit').sort(['Company', 'Year'], ascending=[True, False]).show() #same command using Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = spark.sql(\"select Company, Year, sum(Sales), sum(Profit) from salesDetails group by Company, Year order by sum(Sales) DESC, sum(Profit) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|sum(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "|   APPL|2019|      1845|        201|\n",
      "|   APPL|2018|      1350|        140|\n",
      "|     FB|2018|      1200|        245|\n",
      "|     FB|2019|       748|        215|\n",
      "| GOOGLE|2019|       701|         59|\n",
      "| GOOGLE|2018|       685|        137|\n",
      "| GOOGLE|2017|       575|         95|\n",
      "|   APPL|2017|       505|        100|\n",
      "|     FB|2017|       440|         75|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Sales`' given input columns: [salesdetails.Company, salesdetails.Year, sum(Sales), sum(Profit)]; line 1 pos 125;\\n'Sort ['sum('Sales) DESC NULLS LAST, 'sum('Profit) DESC NULLS LAST], true\\n+- Project [Company#978, Year#980, sum(Sales)#1398L, sum(Profit)#1399L]\\n   +- Filter (sum(cast(Sales#981 as bigint))#1402L > cast(1000 as bigint))\\n      +- Aggregate [Company#978, Year#980], [Company#978, Year#980, sum(cast(Sales#981 as bigint)) AS sum(Sales)#1398L, sum(cast(Profit#982 as bigint)) AS sum(Profit)#1399L, sum(cast(Sales#981 as bigint)) AS sum(cast(Sales#981 as bigint))#1402L]\\n         +- SubqueryAlias `salesdetails`\\n            +- Relation[Company#978,Geo#979,Year#980,Sales#981,Profit#982] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Sales`' given input columns: [salesdetails.Company, salesdetails.Year, sum(Sales), sum(Profit)]; line 1 pos 125;\n'Sort ['sum('Sales) DESC NULLS LAST, 'sum('Profit) DESC NULLS LAST], true\n+- Project [Company#978, Year#980, sum(Sales)#1398L, sum(Profit)#1399L]\n   +- Filter (sum(cast(Sales#981 as bigint))#1402L > cast(1000 as bigint))\n      +- Aggregate [Company#978, Year#980], [Company#978, Year#980, sum(cast(Sales#981 as bigint)) AS sum(Sales)#1398L, sum(cast(Profit#982 as bigint)) AS sum(Profit)#1399L, sum(cast(Sales#981 as bigint)) AS sum(cast(Sales#981 as bigint))#1402L]\n         +- SubqueryAlias `salesdetails`\n            +- Relation[Company#978,Geo#979,Year#980,Sales#981,Profit#982] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-1e76550546a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select Company, Year, sum(Sales), sum(Profit) from salesDetails group by Company, Year having (sum(Sales)>1000) order by sum(Sales) DESC, sum(Profit) DESC\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Not working\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Sales`' given input columns: [salesdetails.Company, salesdetails.Year, sum(Sales), sum(Profit)]; line 1 pos 125;\\n'Sort ['sum('Sales) DESC NULLS LAST, 'sum('Profit) DESC NULLS LAST], true\\n+- Project [Company#978, Year#980, sum(Sales)#1398L, sum(Profit)#1399L]\\n   +- Filter (sum(cast(Sales#981 as bigint))#1402L > cast(1000 as bigint))\\n      +- Aggregate [Company#978, Year#980], [Company#978, Year#980, sum(cast(Sales#981 as bigint)) AS sum(Sales)#1398L, sum(cast(Profit#982 as bigint)) AS sum(Profit)#1399L, sum(cast(Sales#981 as bigint)) AS sum(cast(Sales#981 as bigint))#1402L]\\n         +- SubqueryAlias `salesdetails`\\n            +- Relation[Company#978,Geo#979,Year#980,Sales#981,Profit#982] csv\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Company, Year, sum(Sales), sum(Profit) from salesDetails group by Company, Year having (sum(Sales)>1000) order by sum(Sales) DESC, sum(Profit) DESC\") # Not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|Company|Year|sum(Sales)|sum(Profit)|\n",
      "+-------+----+----------+-----------+\n",
      "|   APPL|2019|      1845|        201|\n",
      "|   APPL|2018|      1350|        140|\n",
      "|     FB|2018|      1200|        245|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results3.filter(results3['sum(Sales)'] > 1000).show() # workaround for the above issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|sum(Sales)|sum(Profit)|\n",
      "+----------+-----------+\n",
      "|      8049|       1267|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy().sum('Sales', 'Profit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|sum(Sales)|sum(Profit)|\n",
      "+----------+-----------+\n",
      "|      8049|       1267|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.agg({'Sales':'sum', 'Profit':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, sqrt, countDistinct, log, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|298.1111111111111|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(avg(df2.Sales)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Sales)|\n",
      "+-----------------+\n",
      "|298.1111111111111|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.agg({'Sales': 'avg'}).show() # using alternate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[stddev_samp(Sales): double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(stddev('Sales')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|standard_deviation|\n",
      "+------------------+\n",
      "|189.54385190470248|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(stddev('Sales').alias('standard_deviation')).show() # stddev is an aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|No._of_Companies|\n",
      "+----------------+\n",
      "|               3|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(countDistinct('Company').alias(\"No._of_Companies\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_sales = df2.select(sqrt('Sales')) # sqrt operates on each value in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       SQRT(Sales)|\n",
      "+------------------+\n",
      "|14.142135623730951|\n",
      "|15.811388300841896|\n",
      "|11.180339887498949|\n",
      "|15.165750888103101|\n",
      "|10.488088481701515|\n",
      "|              10.0|\n",
      "| 11.40175425099138|\n",
      "|14.142135623730951|\n",
      "|13.228756555322953|\n",
      "|15.165750888103101|\n",
      "| 18.16590212458495|\n",
      "|11.180339887498949|\n",
      "|22.360679774997898|\n",
      "|              20.0|\n",
      "|17.320508075688775|\n",
      "| 24.49489742783178|\n",
      "| 23.45207879911715|\n",
      "|14.142135623730951|\n",
      "|11.090536506409418|\n",
      "|              16.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sq_sales.show() # sqrt operates on each value in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------+\n",
      "|count(DISTINCT Sales)|count(DISTINCT Profit)|\n",
      "+---------------------+----------------------+\n",
      "|                   23|                    19|\n",
      "+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(countDistinct(df2['Sales']), countDistinct(df2['Profit'])).show() # aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_profit = df2.select(log('Profit').alias('log_of_profit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|log_of_profit|\n",
      "+-------------+\n",
      "|         3.40|\n",
      "|         3.69|\n",
      "|         3.22|\n",
      "|         3.56|\n",
      "|         2.30|\n",
      "|         3.40|\n",
      "|         3.00|\n",
      "|         3.81|\n",
      "|         3.56|\n",
      "|         2.48|\n",
      "|         3.81|\n",
      "|         4.38|\n",
      "|         3.91|\n",
      "|         4.25|\n",
      "|         4.83|\n",
      "|         4.09|\n",
      "|         3.81|\n",
      "|         3.56|\n",
      "|         3.14|\n",
      "|         2.48|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_profit.select(format_number('log_of_profit', 2).alias('log_of_profit')).show() #format_number function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| Sales|Profit|\n",
      "+------+------+\n",
      "|200.00| 30.00|\n",
      "|250.00| 40.00|\n",
      "|125.00| 25.00|\n",
      "|230.00| 35.00|\n",
      "|110.00| 10.00|\n",
      "|100.00| 30.00|\n",
      "|130.00| 20.00|\n",
      "|200.00| 45.00|\n",
      "|175.00| 35.00|\n",
      "|230.00| 12.00|\n",
      "|330.00| 45.00|\n",
      "|125.00| 80.00|\n",
      "|500.00| 50.00|\n",
      "|400.00| 70.00|\n",
      "|300.00|125.00|\n",
      "|600.00| 60.00|\n",
      "|550.00| 45.00|\n",
      "|200.00| 35.00|\n",
      "|123.00| 23.00|\n",
      "|256.00| 12.00|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(format_number('Sales', 2).alias('Sales'), format_number('Profit', 2).alias(\"Profit\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join and Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = spark.read.csv(\"/media/sf_Ubuntu_Shared/File1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| Name|Sub_Code|Score|\n",
      "+-----+--------+-----+\n",
      "| John|     MAT|   99|\n",
      "| Mary|     SCI|   78|\n",
      "|Louis|     MAT|   89|\n",
      "| Andy|     PSY|   67|\n",
      "|Queen|     SCI|   45|\n",
      "+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='John', Sub_Code='MAT', Score=99),\n",
       " Row(Name='Mary', Sub_Code='SCI', Score=78),\n",
       " Row(Name='Louis', Sub_Code='MAT', Score=89),\n",
       " Row(Name='Andy', Sub_Code='PSY', Score=67),\n",
       " Row(Name='Queen', Sub_Code='SCI', Score=45)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB = spark.read.csv(\"/media/sf_Ubuntu_Shared/File2.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Sub_Code|  Sub_Name|\n",
      "+--------+----------+\n",
      "|     MAT|     Maths|\n",
      "|     SCI|   Science|\n",
      "|     PSY|Psychology|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC = dfA.join(dfB, on='Sub_Code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+-----+\n",
      "| Name|Sub_Code|  Sub_Name|Score|\n",
      "+-----+--------+----------+-----+\n",
      "| John|     MAT|     Maths|   99|\n",
      "| Mary|     SCI|   Science|   78|\n",
      "|Louis|     MAT|     Maths|   89|\n",
      "| Andy|     PSY|Psychology|   67|\n",
      "|Queen|     SCI|   Science|   45|\n",
      "+-----+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfC.select(['Name', 'Sub_Code', 'Sub_Name', 'Score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD = spark.read.csv(\"/media/sf_Ubuntu_Shared/File3.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "| Name|Score|Sub_Code|\n",
      "+-----+-----+--------+\n",
      "|Pilip|   72|     MAT|\n",
      "|Karen|   99|     SCI|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| Name|Sub_Code|Score|\n",
      "+-----+--------+-----+\n",
      "| John|     MAT|   99|\n",
      "| Mary|     SCI|   78|\n",
      "|Louis|     MAT|   89|\n",
      "| Andy|     PSY|   67|\n",
      "|Queen|     SCI|   45|\n",
      "|Pilip|     MAT|   72|\n",
      "|Karen|     SCI|   99|\n",
      "+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA.unionByName(dfD).show() # equivalent of append() in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfE = spark.read.csv(\"/media/sf_Ubuntu_Shared/File_dup.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfE.count() # before de-duping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfE = dfE.distinct() # remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfE.count() # after de-duping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/media/sf_Ubuntu_Shared/Missing_Data.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "| Name|   Age|Salary|\n",
      "+-----+------+------+\n",
      "| John|    NA|  43.5|\n",
      "| None|    23|  20.5|\n",
      "|Louis|    45|  null|\n",
      "| Sean|    22|  67.8|\n",
      "|  NaN|np.nan|  23.5|\n",
      "| Bill|   nan|   122|\n",
      "| null|    12|   NAN|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|Name|   Age|Salary|\n",
      "+----+------+------+\n",
      "|John|    NA|  43.5|\n",
      "|None|    23|  20.5|\n",
      "|Sean|    22|  67.8|\n",
      "| NaN|np.nan|  23.5|\n",
      "|Bill|   nan|   122|\n",
      "|null|    12|   NAN|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, isnan, mean, format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------------+\n",
      "|(Name IS NULL)|(Age IS NULL)|(Salary IS NULL)|\n",
      "+--------------+-------------+----------------+\n",
      "|         false|        false|           false|\n",
      "|         false|        false|           false|\n",
      "|         false|        false|            true|\n",
      "|         false|        false|           false|\n",
      "|         false|        false|           false|\n",
      "|         false|        false|           false|\n",
      "|         false|        false|           false|\n",
      "+--------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(isnull('Name'), isnull('Age'), isnull('Salary')).show() # only empty cell in the input file is considered as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Salary|count|\n",
      "+------+-----+\n",
      "|  true|    1|\n",
      "| false|    6|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(isnull('Salary').alias(\"Salary\")).groupBy('Salary').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+\n",
      "|isnan(Name)|isnan(Age)|isnan(Salary)|\n",
      "+-----------+----------+-------------+\n",
      "|      false|     false|        false|\n",
      "|      false|     false|        false|\n",
      "|      false|     false|        false|\n",
      "|      false|     false|        false|\n",
      "|       true|     false|        false|\n",
      "|      false|     false|        false|\n",
      "|      false|     false|        false|\n",
      "+-----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(isnan('Name'), isnan('Age'), isnan('Salary')).show() # null is different from nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.replace(['NA', 'NaN', 'nan', 'np.nan', 'None', 'NAN', 'null' ], value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| Name| Age|Salary|\n",
      "+-----+----+------+\n",
      "| John|null|  43.5|\n",
      "| null|  23|  20.5|\n",
      "|Louis|  45|  null|\n",
      "| Sean|  22|  67.8|\n",
      "| null|null|  23.5|\n",
      "| Bill|null|   122|\n",
      "| null|  12|  null|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+----------------+\n",
      "|(Name IS NULL)|(Age IS NULL)|(Salary IS NULL)|\n",
      "+--------------+-------------+----------------+\n",
      "|         false|         true|           false|\n",
      "|          true|        false|           false|\n",
      "|         false|        false|            true|\n",
      "|         false|        false|           false|\n",
      "|          true|         true|           false|\n",
      "|         false|         true|           false|\n",
      "|          true|        false|            true|\n",
      "+--------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(isnull('Name'), isnull('Age'), isnull('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='John', Age=None, Salary='43.5'),\n",
       " Row(Name=None, Age='23', Salary='20.5'),\n",
       " Row(Name='Louis', Age='45', Salary=None),\n",
       " Row(Name='Sean', Age='22', Salary='67.8'),\n",
       " Row(Name=None, Age=None, Salary='23.5'),\n",
       " Row(Name='Bill', Age=None, Salary='122'),\n",
       " Row(Name=None, Age='12', Salary=None)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.collect() # In the Row objects, null is represented as None\n",
    "# Since null value is present, numeric data is stored as string, which we want to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'string'), ('Salary', 'string')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes # Note that due presence of null values, numeric values are also stored as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2.select(df2.Name, df2.Age.cast(IntegerType()), df2.Salary.cast(FloatType())).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='John', Age=None, Salary=43.5),\n",
       " Row(Name=None, Age=23, Salary=20.5),\n",
       " Row(Name='Louis', Age=45, Salary=None),\n",
       " Row(Name='Sean', Age=22, Salary=67.80000305175781),\n",
       " Row(Name=None, Age=None, Salary=23.5),\n",
       " Row(Name='Bill', Age=None, Salary=122.0),\n",
       " Row(Name=None, Age=12, Salary=None)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataschema = StructType([StructField('Name', StringType(), True), StructField('Age', IntegerType(), True),\n",
    "                        StructField('Salary', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(data, schema=dataschema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='John', Age=None, Salary=43.5),\n",
       " Row(Name=None, Age=23, Salary=20.5),\n",
       " Row(Name='Louis', Age=45, Salary=None),\n",
       " Row(Name='Sean', Age=22, Salary=67.80000305175781),\n",
       " Row(Name=None, Age=None, Salary=23.5),\n",
       " Row(Name='Bill', Age=None, Salary=122.0),\n",
       " Row(Name=None, Age=12, Salary=None)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.collect() #Now the data type for numeric columns is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(format_number(Salary, 2)='43.50'),\n",
       " Row(format_number(Salary, 2)='20.50'),\n",
       " Row(format_number(Salary, 2)=None),\n",
       " Row(format_number(Salary, 2)='67.80'),\n",
       " Row(format_number(Salary, 2)='23.50'),\n",
       " Row(format_number(Salary, 2)='122.00'),\n",
       " Row(format_number(Salary, 2)=None)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.select(format_number('Salary', 2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|Name|Age|Salary|\n",
      "+----+---+------+\n",
      "|Sean| 22|  67.8|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Salary', 'float')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.dtypes # Note that the data types are correct now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = int(df3.select(mean('Age')).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_salary = df3.approxQuantile('Salary', [0.5], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.5]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------------+\n",
      "|   Name|Age|           Salary|\n",
      "+-------+---+-----------------+\n",
      "|   John| 25|             43.5|\n",
      "|unknown| 23|             20.5|\n",
      "|  Louis| 45|             43.5|\n",
      "|   Sean| 22|67.80000305175781|\n",
      "|unknown| 25|             23.5|\n",
      "|   Bill| 25|            122.0|\n",
      "|unknown| 12|             43.5|\n",
      "+-------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.fillna({'Name':'unknown', 'Age':mean_age, 'Salary':median_salary[0]}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (to_date, year, format_number, month, dayofmonth, \n",
    "                                   dayofweek, dayofyear, weekofyear, quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/media/sf_Ubuntu_Shared/appl_stock.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+----------+---------+---------+\n",
      "|    Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+--------+----------+----------+----------+----------+---------+---------+\n",
      "|04/01/10|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|05/01/10|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|06/01/10|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "|07/01/10|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|\n",
      "|08/01/10|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|\n",
      "+--------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'string'),\n",
       " ('Open', 'double'),\n",
       " ('High', 'double'),\n",
       " ('Low', 'double'),\n",
       " ('Close', 'double'),\n",
       " ('Volume', 'int'),\n",
       " ('Adj Close', 'double')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('Date_new', to_date(df1.Date, format='dd/MM/yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(df2.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+---------+---------+----------+\n",
      "|      Open|      High|       Low|     Close|   Volume|Adj Close|  Date_new|\n",
      "+----------+----------+----------+----------+---------+---------+----------+\n",
      "|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|2010-01-04|\n",
      "|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|2010-01-05|\n",
      "|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|2010-01-06|\n",
      "|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|2010-01-07|\n",
      "|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|2010-01-08|\n",
      "+----------+----------+----------+----------+---------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Open', 'double'),\n",
       " ('High', 'double'),\n",
       " ('Low', 'double'),\n",
       " ('Close', 'double'),\n",
       " ('Volume', 'int'),\n",
       " ('Adj Close', 'double'),\n",
       " ('Date_new', 'date')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Open=213.429998, High=214.499996, Low=212.380001, Close=214.009998, Volume=123432400, Adj Close=27.727039, Date_new=datetime.date(2010, 1, 4)),\n",
       " Row(Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976, Date_new=datetime.date(2010, 1, 5)),\n",
       " Row(Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178, Date_new=datetime.date(2010, 1, 6)),\n",
       " Row(Open=211.75, High=212.000006, Low=209.050005, Close=210.58, Volume=119282800, Adj Close=27.28265, Date_new=datetime.date(2010, 1, 7)),\n",
       " Row(Open=210.299994, High=212.000006, Low=209.060005, Close=211.980005, Volume=111902700, Adj Close=27.464034, Date_new=datetime.date(2010, 1, 8))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2010, 1, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_list[0]['Date_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('Year', year(df2.Date_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+---------+---------+----------+----+\n",
      "|      Open|      High|       Low|     Close|   Volume|Adj Close|  Date_new|Year|\n",
      "+----------+----------+----------+----------+---------+---------+----------+----+\n",
      "|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|2010-01-04|2010|\n",
      "|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|2010-01-05|2010|\n",
      "|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|2010-01-06|2010|\n",
      "|    211.75|212.000006|209.050005|    210.58|119282800| 27.28265|2010-01-07|2010|\n",
      "|210.299994|212.000006|209.060005|211.980005|111902700|27.464034|2010-01-08|2010|\n",
      "|212.799997|213.000002|208.450005|210.110003|115557400|27.221758|2010-01-11|2010|\n",
      "|209.189995|209.769995|206.419998|207.720001|148614900| 26.91211|2010-01-12|2010|\n",
      "|207.870005|210.929995|204.099998|210.650002|151473000| 27.29172|2010-01-13|2010|\n",
      "|210.110003|210.459997|209.020004|    209.43|108223500|27.133657|2010-01-14|2010|\n",
      "|210.929995|211.599997|205.869999|    205.93|148516900|26.680198|2010-01-15|2010|\n",
      "|208.330002|215.189999|207.240004|215.039995|182501900|27.860485|2010-01-19|2010|\n",
      "|214.910006|215.549994|209.500002|    211.73|153038200|27.431644|2010-01-20|2010|\n",
      "|212.079994|213.309996|207.210003|208.069996|152038600|26.957455|2010-01-21|2010|\n",
      "|206.780006|207.499996|    197.16|    197.75|220441900|25.620401|2010-01-22|2010|\n",
      "|202.510002|204.699999|200.190002|203.070002|266424900|26.309658|2010-01-25|2010|\n",
      "|205.950001|213.710005|202.580004|205.940001|466777500|26.681494|2010-01-26|2010|\n",
      "|206.849995|    210.58|199.530001|207.880005|430642100| 26.93284|2010-01-27|2010|\n",
      "|204.930004|205.500004|198.699995|199.289995|293375600|25.819922|2010-01-28|2010|\n",
      "|201.079996|202.199995|190.250002|192.060003|311488100|24.883208|2010-01-29|2010|\n",
      "|192.369997|     196.0|191.299999|194.729998|187469100|25.229131|2010-02-01|2010|\n",
      "+----------+----------+----------+----------+---------+---------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.groupBy('Year').avg('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2015|120.03999980555547|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2012| 576.0497195640002|\n",
      "|2016|104.60400786904763|\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|Year|avgClose|\n",
      "+----+--------+\n",
      "|2016|  103.62|\n",
      "|2015|  120.04|\n",
      "|2014|  295.40|\n",
      "|2013|  472.63|\n",
      "|2012|  576.05|\n",
      "|2011|  364.00|\n",
      "|2010|  259.84|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select(['Year', format_number(df4['avg(Close)'], 2).alias('avgClose')]).orderBy(df4['Year'], ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = spark.createDataFrame(data=[(5,), (3,), (12,)], schema=['Age']) # sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "|  5|\n",
      "|  3|\n",
      "| 12|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = spark.createDataFrame(data=[(row_list[0]['Date_new'],), (row_list[100]['Date_new'],)], schema=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2010-01-04|\n",
      "|2010-05-27|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2010|\n",
      "|2010|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(year(DF2.Date).alias(\"Year\")).show() # year function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Month|\n",
      "+-----+\n",
      "|    1|\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(month(DF2.Date).alias(\"Month\")).show() # month function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Day of Month|\n",
      "+------------+\n",
      "|           4|\n",
      "|          27|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(dayofmonth(DF2.Date).alias(\"Day of Month\")).show() # day function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Day of Year|\n",
      "+-----------+\n",
      "|          4|\n",
      "|        147|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(dayofyear(DF2.Date).alias(\"Day of Year\")).show() # day function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Day of Week|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(dayofweek(DF2.Date).alias(\"Day of Week\")).show() # day function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Week|\n",
      "+----+\n",
      "|   1|\n",
      "|  21|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(weekofyear(DF2.Date).alias(\"Week\")).show() # week function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Quarter|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.select(quarter(DF2.Date).alias(\"Quarter\")).show() # quarter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the appl_stock.csv after fixing date format error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = spark.read.csv(\"/media/sf_Ubuntu_Shared/appl_stock.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'timestamp'),\n",
       " ('Open', 'double'),\n",
       " ('High', 'double'),\n",
       " ('Low', 'double'),\n",
       " ('Close', 'double'),\n",
       " ('Volume', 'int'),\n",
       " ('Adj Close', 'double')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF2.dtypes #note that the correct dtype has been assigned to Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+---------+---------+\n",
      "|               Date|      Open|      High|       Low|     Close|   Volume|Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+---------+---------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.380001|214.009998|123432400|27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|213.249994|214.379993|150476200|27.774976|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|210.750004|210.969995|138040000|27.333178|\n",
      "+-------------------+----------+----------+----------+----------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = DF2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.380001, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2010, 1, 4, 0, 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_list[0]['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data =[(3, 7), (6, 2), (1, 9)], \n",
    "                           schema = ['A', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  3|  7|\n",
      "|  6|  2|\n",
      "|  1|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    return (x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = pandas_udf(f=f1, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|f1(A, B)|\n",
      "+--------+\n",
      "|      58|\n",
      "|      40|\n",
      "|      82|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(f2(col('A'), col('B'))).show() # use pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select((df1.A > df1.B).alias('col1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| col1|\n",
      "+-----+\n",
      "|false|\n",
      "| true|\n",
      "|false|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "| 0.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(df2.col1.cast(FloatType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.select((df1.A > t).alias('col'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  col|\n",
      "+-----+\n",
      "|false|\n",
      "| true|\n",
      "|false|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(df3.col.cast(IntegerType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
